{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0cApO6fOho_"
      },
      "source": [
        "I took things out of `training.py` script so I can easily modify them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVsHdYcsOhpH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yUNHf_Vsxu2",
        "outputId": "8d6e58c1-d5c1-4006-f16e-5cd52f7ae15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.43.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlgiYZ-yOhpH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from tensorflow.keras.layers import (\n",
        "                                BatchNormalization, LeakyReLU,\n",
        "                                Input, Dense, Conv2D,\n",
        "                                MaxPooling2D, Flatten, Dropout)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_addons as tfa\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlIxFZ0POhpL"
      },
      "outputs": [],
      "source": [
        "# set random state for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmKjch2OhpJ"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psqRgujqOhpK"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(df, tensor_dim=(50, 20, 1)):\n",
        "    \"\"\"\n",
        "    fun transform input database to numpy array.\n",
        "    \n",
        "    parameters:\n",
        "    df = Pandas df with col names \"gene\", \"label\", \"miRNA\"\n",
        "    tensor_dim= 2d matrix shape\n",
        "    \n",
        "    output:\n",
        "    2d dot matrix, labels as np array\n",
        "    \"\"\"\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # alphabet for watson-crick interactions.\n",
        "    alphabet = {\"AT\": 1., \"TA\": 1., \"GC\": 1., \"CG\": 1.}\n",
        "\n",
        "    # labels to one hot encoding\n",
        "    label = df[\"label\"].to_numpy()\n",
        "\n",
        "    # create empty main 2d matrix array\n",
        "    N = df.shape[0]  # number of samples in df\n",
        "    shape_matrix_2d = (N, *tensor_dim)  # 2d matrix shape\n",
        "    # initialize dot matrix with zeros\n",
        "    ohe_matrix_2d = np.zeros(shape_matrix_2d, dtype=\"float32\")\n",
        "\n",
        "    # compile matrix with watson-crick interactions.\n",
        "    for index, row in df.iterrows():\n",
        "        for bind_index, bind_nt in enumerate(row.gene.upper()):\n",
        "\n",
        "            for mirna_index, mirna_nt in enumerate(row.miRNA.upper()):\n",
        "\n",
        "                base_pairs = bind_nt + mirna_nt\n",
        "                ohe_matrix_2d[index, bind_index, mirna_index, 0] = alphabet.get(base_pairs, 0)\n",
        "\n",
        "    return ohe_matrix_2d, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(training_ratio, train=True):\n",
        "  if train == True:\n",
        "    part = 'train'\n",
        "  else:\n",
        "    part = 'evaluation'\n",
        "  df = pd.read_csv(\"../Datasets/\" + part + \"_set_1_\" + str(training_ratio) + \"_CLASH2013_paper.tsv\", sep='\\t')\n",
        "  df = df.sample(frac=1, random_state=RANDOM_STATE)\n",
        "  ohe_data = one_hot_encoding(df)\n",
        "  ohe, labels = ohe_data\n",
        "  return ohe, labels"
      ],
      "metadata": {
        "id": "x68rhGGtpK2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell in Google Colab to get the data into right place\n",
        "!mkdir ../Datasets\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_1_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_10_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_100_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_1_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_10_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_100_CLASH2013_paper.tsv -P ../Datasets/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSuBm8RnpdD0",
        "outputId": "a5312865-dc3f-4f6c-92c3-f442e4d473b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-24 14:10:44--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_1_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2276853 (2.2M) [text/plain]\n",
            "Saving to: ‘../Datasets/train_set_1_1_CLASH2013_paper.tsv’\n",
            "\n",
            "train_set_1_1_CLASH 100%[===================>]   2.17M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-02-24 14:10:44 (35.5 MB/s) - ‘../Datasets/train_set_1_1_CLASH2013_paper.tsv’ saved [2276853/2276853]\n",
            "\n",
            "--2022-02-24 14:10:44--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_10_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12518906 (12M) [text/plain]\n",
            "Saving to: ‘../Datasets/train_set_1_10_CLASH2013_paper.tsv’\n",
            "\n",
            "train_set_1_10_CLAS 100%[===================>]  11.94M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-24 14:10:45 (121 MB/s) - ‘../Datasets/train_set_1_10_CLASH2013_paper.tsv’ saved [12518906/12518906]\n",
            "\n",
            "--2022-02-24 14:10:45--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_100_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 134 [text/plain]\n",
            "Saving to: ‘../Datasets/train_set_1_100_CLASH2013_paper.tsv’\n",
            "\n",
            "train_set_1_100_CLA 100%[===================>]     134  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-24 14:10:45 (6.37 MB/s) - ‘../Datasets/train_set_1_100_CLASH2013_paper.tsv’ saved [134/134]\n",
            "\n",
            "--2022-02-24 14:10:45--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_1_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147946 (144K) [text/plain]\n",
            "Saving to: ‘../Datasets/evaluation_set_1_1_CLASH2013_paper.tsv’\n",
            "\n",
            "evaluation_set_1_1_ 100%[===================>] 144.48K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-24 14:10:45 (6.56 MB/s) - ‘../Datasets/evaluation_set_1_1_CLASH2013_paper.tsv’ saved [147946/147946]\n",
            "\n",
            "--2022-02-24 14:10:45--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_10_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 813358 (794K) [text/plain]\n",
            "Saving to: ‘../Datasets/evaluation_set_1_10_CLASH2013_paper.tsv’\n",
            "\n",
            "evaluation_set_1_10 100%[===================>] 794.29K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-02-24 14:10:46 (18.0 MB/s) - ‘../Datasets/evaluation_set_1_10_CLASH2013_paper.tsv’ saved [813358/813358]\n",
            "\n",
            "--2022-02-24 14:10:46--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_100_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7467752 (7.1M) [text/plain]\n",
            "Saving to: ‘../Datasets/evaluation_set_1_100_CLASH2013_paper.tsv’\n",
            "\n",
            "evaluation_set_1_10 100%[===================>]   7.12M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-02-24 14:10:46 (91.0 MB/s) - ‘../Datasets/evaluation_set_1_100_CLASH2013_paper.tsv’ saved [7467752/7467752]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Mn0Q3LOhpP"
      },
      "source": [
        "# Model + Keras Tuner setup\n",
        "\n",
        "Following this tutorial https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html to setup hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATFtpQ22OhpQ"
      },
      "outputs": [],
      "source": [
        "def make_architecture(hp):\n",
        "    \"\"\"\n",
        "    build model architecture\n",
        "\n",
        "    return a model object\n",
        "    \"\"\"\n",
        "    x = Input(shape=(50, 20, 1),\n",
        "                       dtype='float32', name='main_input'\n",
        "                       )\n",
        "    main_input = x\n",
        "\n",
        "    cnn_num = hp.Int('conv_blocks', 2, 6, default=3)\n",
        "    kernel_size = hp.Int('kernel_size', 3, 6, default=3)\n",
        "    pool_size = hp.Int('pool_size', 2, 5, default=2)\n",
        "    dropout_rate = hp.Float('dropout', 0, 0.6, step=0.05, default=0.25)\n",
        "    # max the same number of dense layers as is the number of cnn layers\n",
        "    dense_num = hp.Int('dense_blocks', 2, cnn_num, default=3)\n",
        "\n",
        "    for cnn_i in range(cnn_num):\n",
        "        x = Conv2D(\n",
        "            # we increase number of filters by 32 in each layer\n",
        "            filters=32 * (cnn_i + 1),\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            padding=\"same\",\n",
        "            data_format=\"channels_last\",\n",
        "            name=\"conv_\" + str(cnn_i + 1))(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(pool_size, pool_size), padding='same', name='Max_' + str(cnn_i + 1))(x)\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    x = Flatten(name='2d_matrix')(x)\n",
        "\n",
        "    for dense_i in range(dense_num):\n",
        "        neurons = 32 * (cnn_num - dense_i)\n",
        "        x = Dense(neurons)(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
        "\n",
        "    m = K.Model(inputs=[main_input], outputs=[main_output], name='arch_00')\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWiDvoooOhpR"
      },
      "outputs": [],
      "source": [
        "def compile_model(hp):\n",
        "    K.backend.clear_session()\n",
        "    m = make_architecture(hp)\n",
        "\n",
        "    opt = Adam(\n",
        "        learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'),\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name=\"Adam\")\n",
        "    \n",
        "    binary_f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5, average=\"micro\")\n",
        "    m.compile(\n",
        "        optimizer=opt,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', binary_f1_score, K.metrics.AUC(name='prc', curve='PR')]\n",
        "        )\n",
        "    return m"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tune"
      ],
      "metadata": {
        "id": "0UJefeyJsB-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {}\n",
        "\n",
        "for ratio in [1, 10, 100]:\n",
        "    tuner = kt.BayesianOptimization(\n",
        "      compile_model,\n",
        "      objective=kt.Objective('val_prc', direction='max'),\n",
        "      max_trials=100,\n",
        "      seed=42,\n",
        "      project_name='miRBind_' + str(ratio)\n",
        "    )\n",
        "\n",
        "    train_ohe, train_labels = load_data(ratio, train=True)\n",
        "    val_ohe, val_labels = load_data(ratio, train=False)\n",
        "\n",
        "    tuner.search(train_ohe, train_labels,\n",
        "                 validation_data=(val_ohe, val_labels),\n",
        "                 epochs=10,\n",
        "                 callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
        "                 batch_size=32,\n",
        "                 class_weight={0: 1, 1: ratio}\n",
        "                 )\n",
        "\n",
        "    print(\"Best params for ratio \", ratio, ' are: ', tuner.get_best_hyperparameters(1)[0].values)\n",
        "    best_params[ratio] = tuner.get_best_hyperparameters(1)[0].values\n",
        "    best_model = tuner.get_best_models(1)[0]\n",
        "    best_model.save(\"model_1_\" + str(ratio) + \".h5\")\n",
        "\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "dNxA2f-lwKjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "dc8fa1878b8b3108891aa0a714ae13672a5fe7404eec961f879aba1551dafbcb"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "hyperparams.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}