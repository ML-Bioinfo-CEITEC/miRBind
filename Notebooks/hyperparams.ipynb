{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0cApO6fOho_"
      },
      "source": [
        "I took things out of `training.py` script so I can easily modify them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVsHdYcsOhpH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yUNHf_Vsxu2",
        "outputId": "4e9f772c-215a-40a1-d2b0-0e0447ec9aef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TlgiYZ-yOhpH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from tensorflow.keras.layers import (\n",
        "                                BatchNormalization, LeakyReLU,\n",
        "                                Input, Dense, Conv2D,\n",
        "                                MaxPooling2D, Flatten, Dropout)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_addons as tfa\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RlIxFZ0POhpL"
      },
      "outputs": [],
      "source": [
        "# set random state for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmKjch2OhpJ"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "psqRgujqOhpK"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(df, tensor_dim=(50, 20, 1)):\n",
        "    \"\"\"\n",
        "    fun transform input database to numpy array.\n",
        "    \n",
        "    parameters:\n",
        "    df = Pandas df with col names \"gene\", \"label\", \"miRNA\"\n",
        "    tensor_dim= 2d matrix shape\n",
        "    \n",
        "    output:\n",
        "    2d dot matrix, labels as np array\n",
        "    \"\"\"\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # alphabet for watson-crick interactions.\n",
        "    alphabet = {\"AT\": 1., \"TA\": 1., \"GC\": 1., \"CG\": 1.}\n",
        "\n",
        "    # labels to one hot encoding\n",
        "    label = df[\"label\"].to_numpy()\n",
        "\n",
        "    # create empty main 2d matrix array\n",
        "    N = df.shape[0]  # number of samples in df\n",
        "    shape_matrix_2d = (N, *tensor_dim)  # 2d matrix shape\n",
        "    # initialize dot matrix with zeros\n",
        "    ohe_matrix_2d = np.zeros(shape_matrix_2d, dtype=\"float32\")\n",
        "\n",
        "    # compile matrix with watson-crick interactions.\n",
        "    for index, row in df.iterrows():\n",
        "        for bind_index, bind_nt in enumerate(row.gene.upper()):\n",
        "\n",
        "            for mirna_index, mirna_nt in enumerate(row.miRNA.upper()):\n",
        "\n",
        "                base_pairs = bind_nt + mirna_nt\n",
        "                ohe_matrix_2d[index, bind_index, mirna_index, 0] = alphabet.get(base_pairs, 0)\n",
        "\n",
        "    return ohe_matrix_2d, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(training_ratio, train=True):\n",
        "  if train == True:\n",
        "    part = 'train'\n",
        "  else:\n",
        "    part = 'evaluation'\n",
        "  df = pd.read_csv(\"../Datasets/\" + part + \"_set_1_\" + str(training_ratio) + \"_CLASH2013_paper.tsv\", sep='\\t')\n",
        "  df = df.sample(frac=1, random_state=RANDOM_STATE)\n",
        "  ohe_data = one_hot_encoding(df)\n",
        "  ohe, labels = ohe_data\n",
        "  return ohe, labels"
      ],
      "metadata": {
        "id": "x68rhGGtpK2s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell in Google Colab to get the data into right place\n",
        "!mkdir ../Datasets\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_1_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_10_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_100_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_1_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_10_CLASH2013_paper.tsv -P ../Datasets/\n",
        "!wget https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_100_CLASH2013_paper.tsv -P ../Datasets/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSuBm8RnpdD0",
        "outputId": "9f141c41-700b-4168-f1ca-118e8412f58b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘../Datasets’: File exists\n",
            "--2022-02-24 12:28:27--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_1_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2276853 (2.2M) [text/plain]\n",
            "Saving to: ‘../Datasets/train_set_1_1_CLASH2013_paper.tsv.1’\n",
            "\n",
            "train_set_1_1_CLASH 100%[===================>]   2.17M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-02-24 12:28:27 (29.2 MB/s) - ‘../Datasets/train_set_1_1_CLASH2013_paper.tsv.1’ saved [2276853/2276853]\n",
            "\n",
            "--2022-02-24 12:28:27--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_10_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12518906 (12M) [text/plain]\n",
            "Saving to: ‘../Datasets/train_set_1_10_CLASH2013_paper.tsv.1’\n",
            "\n",
            "train_set_1_10_CLAS 100%[===================>]  11.94M  71.3MB/s    in 0.2s    \n",
            "\n",
            "2022-02-24 12:28:28 (71.3 MB/s) - ‘../Datasets/train_set_1_10_CLASH2013_paper.tsv.1’ saved [12518906/12518906]\n",
            "\n",
            "--2022-02-24 12:28:28--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/train_set_1_100_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 134 [text/plain]\n",
            "Saving to: ‘../Datasets/train_set_1_100_CLASH2013_paper.tsv.1’\n",
            "\n",
            "train_set_1_100_CLA 100%[===================>]     134  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-24 12:28:28 (4.12 MB/s) - ‘../Datasets/train_set_1_100_CLASH2013_paper.tsv.1’ saved [134/134]\n",
            "\n",
            "--2022-02-24 12:28:28--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_1_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147946 (144K) [text/plain]\n",
            "Saving to: ‘../Datasets/evaluation_set_1_1_CLASH2013_paper.tsv.1’\n",
            "\n",
            "evaluation_set_1_1_ 100%[===================>] 144.48K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-02-24 12:28:28 (4.38 MB/s) - ‘../Datasets/evaluation_set_1_1_CLASH2013_paper.tsv.1’ saved [147946/147946]\n",
            "\n",
            "--2022-02-24 12:28:28--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_10_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 813358 (794K) [text/plain]\n",
            "Saving to: ‘../Datasets/evaluation_set_1_10_CLASH2013_paper.tsv.1’\n",
            "\n",
            "evaluation_set_1_10 100%[===================>] 794.29K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-24 12:28:28 (14.6 MB/s) - ‘../Datasets/evaluation_set_1_10_CLASH2013_paper.tsv.1’ saved [813358/813358]\n",
            "\n",
            "--2022-02-24 12:28:28--  https://raw.githubusercontent.com/ML-Bioinfo-CEITEC/miRBind/tuning/Datasets/evaluation_set_1_100_CLASH2013_paper.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7467752 (7.1M) [text/plain]\n",
            "Saving to: ‘../Datasets/evaluation_set_1_100_CLASH2013_paper.tsv.1’\n",
            "\n",
            "evaluation_set_1_10 100%[===================>]   7.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-24 12:28:29 (64.3 MB/s) - ‘../Datasets/evaluation_set_1_100_CLASH2013_paper.tsv.1’ saved [7467752/7467752]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Mn0Q3LOhpP"
      },
      "source": [
        "# Model + Keras Tuner setup\n",
        "\n",
        "Following this tutorial https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html to setup hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ATFtpQ22OhpQ"
      },
      "outputs": [],
      "source": [
        "def make_architecture(hp):\n",
        "    \"\"\"\n",
        "    build model architecture\n",
        "\n",
        "    return a model object\n",
        "    \"\"\"\n",
        "    main_input = Input(shape=(50, 20, 1),\n",
        "                       dtype='float32', name='main_input'\n",
        "                       )\n",
        "\n",
        "    cnn_num = hp.Int('conv_blocks', 2, 6, default=3)\n",
        "    kernel_size = hp.Int('kernel_size', 3, 6, default=3)\n",
        "    pool_size = hp.Int('pool_size', 2, 5, default=2)\n",
        "    dropout_rate = hp.Float('dropout', 0, 0.6, step=0.05, default=0.25)\n",
        "    # max the same number of dense layers as is the number of cnn layers\n",
        "    dense_num = hp.Int('dense_blocks', 2, cnn_num, default=3)\n",
        "\n",
        "\n",
        "    for cnn_i in range(cnn_num):\n",
        "\n",
        "        x = Conv2D(\n",
        "            # we increase number of filters by 32 in each layer\n",
        "            filters=32*(cnn_i + 1),\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            padding=\"same\",\n",
        "            data_format=\"channels_last\",\n",
        "            name=\"conv_\" + str(cnn_i + 1))(main_input)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(pool_size, pool_size), name='Max_' + str(cnn_i + 1))(x)\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    conv_flat = Flatten(name='2d_matrix')(x)\n",
        "\n",
        "    for dense_i in range(dense_num):\n",
        "\n",
        "        neurons = 32 * (cnn_num - dense_i)\n",
        "        x = Dense(neurons)(conv_flat)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
        "\n",
        "    m = K.Model(inputs=[main_input], outputs=[main_output], name='arch_00')\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AWiDvoooOhpR"
      },
      "outputs": [],
      "source": [
        "def compile_model(hp):\n",
        "    K.backend.clear_session()\n",
        "    m = make_architecture(hp)\n",
        "\n",
        "    opt = Adam(\n",
        "        learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'),\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name=\"Adam\")\n",
        "    \n",
        "    binary_f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5, average=\"micro\")\n",
        "    m.compile(\n",
        "        optimizer=opt,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', binary_f1_score, K.metrics.AUC(name='prc', curve='PR')]\n",
        "        )\n",
        "    return m"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tune"
      ],
      "metadata": {
        "id": "0UJefeyJsB-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ratio in [1, 10, 100]:\n",
        "\n",
        "  tuner = kt.Hyperband(\n",
        "    compile_model,\n",
        "    objective=kt.Objective('val_prc', direction='max'),\n",
        "    max_epochs=30,\n",
        "    hyperband_iterations=2,\n",
        "    project_name='miRBind_' + str(ratio)\n",
        "  )\n",
        "\n",
        "  train_ohe, train_labels = load_data(1, train=True)\n",
        "  val_ohe, val_labels = load_data(1, train=False)\n",
        "\n",
        "  tuner.search(train_ohe, train_labels,\n",
        "             validation_data=(val_ohe, val_labels),\n",
        "             epochs=10,\n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
        "             batch_size=32,\n",
        "            class_weight={0: 1, 1: ratio}\n",
        "  )\n",
        "  \n",
        "  print(\"Best params for ratio\", ratio, \" are: \", tuner.get_best_hyperparameters(1)[0])\n",
        "\n",
        "  best_model = tuner.get_best_models(1)[0]\n",
        "  best_model.save(\"model_1_\" + str(ratio) + \".h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNxA2f-lwKjZ",
        "outputId": "3fd4ee6d-5f0c-4403-c637-e927f60e47a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 68 Complete [00h 01m 27s]\n",
            "val_prc: 0.9222705364227295\n",
            "\n",
            "Best val_prc So Far: 0.9442317485809326\n",
            "Total elapsed time: 00h 46m 37s\n",
            "\n",
            "Search: Running Trial #69\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "conv_blocks       |3                 |5                 \n",
            "kernel_size       |4                 |6                 \n",
            "pool_size         |5                 |5                 \n",
            "dropout           |0.1               |0.5               \n",
            "dense_blocks      |2                 |3                 \n",
            "learning_rate     |0.0060713         |0.00065915        \n",
            "tuner/epochs      |10                |30                \n",
            "tuner/initial_e...|4                 |10                \n",
            "tuner/bracket     |2                 |3                 \n",
            "tuner/round       |1                 |3                 \n",
            "tuner/trial_id    |fbf57473180be3e...|55e36025a0b211c...\n",
            "\n",
            "Epoch 5/10\n",
            "962/962 [==============================] - 10s 9ms/step - loss: 0.4940 - accuracy: 0.7693 - f1_score: 0.7676 - prc: 0.8416 - val_loss: 0.4291 - val_accuracy: 0.8120 - val_f1_score: 0.8105 - val_prc: 0.8903\n",
            "Epoch 6/10\n",
            "962/962 [==============================] - 9s 9ms/step - loss: 0.4341 - accuracy: 0.8047 - f1_score: 0.8025 - prc: 0.8841 - val_loss: 0.5015 - val_accuracy: 0.7575 - val_f1_score: 0.7953 - val_prc: 0.8977\n",
            "Epoch 7/10\n",
            "  1/962 [..............................] - ETA: 11s - loss: 0.4364 - accuracy: 0.8438 - f1_score: 0.8485 - prc: 0.7826"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "dc8fa1878b8b3108891aa0a714ae13672a5fe7404eec961f879aba1551dafbcb"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "hyperparams.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}